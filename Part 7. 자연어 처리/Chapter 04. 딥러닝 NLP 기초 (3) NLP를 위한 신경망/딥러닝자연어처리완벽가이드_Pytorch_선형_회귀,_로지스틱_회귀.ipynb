{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5iHhbYjxn9N"
      },
      "source": [
        "# 1. 선형 회귀 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nslr5Q7hX1Q8"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjJnWfNRX9Zt",
        "outputId": "8d35dbbf-bf31-45ed-c488-be2db7f4a2c3"
      },
      "source": [
        "torch.manual_seed(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fab0c7dc3b0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "실습을 위한 기본적인 셋팅이 끝났습니다. 이제 훈련 데이터인 x_train과 y_train을 선언합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "rKqpBOFR_YhF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1YtJLENYGUO"
      },
      "source": [
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "x_train과 x_train의 크기(shape)를 출력해보겠습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "BeEHeOkg_aLu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctuxU-qzYIpu",
        "outputId": "43c2ed9f-b364-4054-9561-24f4361fc1af"
      },
      "source": [
        "print(x_train)\n",
        "print(x_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.]])\n",
            "torch.Size([3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "x_train의 값이 출력되고, x_train의 크기가 (3 × 1)임을 알 수 있습니다.\n",
        "y_train과 y_train의 크기(shape)를 출력해보겠습니다."
      ],
      "metadata": {
        "id": "vmSvtigK_cbM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNg7dcxUYMAR",
        "outputId": "6309d657-9560-4050-a145-3d44ab2a48fb"
      },
      "source": [
        "print(y_train)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.],\n",
            "        [4.],\n",
            "        [6.]])\n",
            "torch.Size([3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "y_train의 값이 출력되고, y_train의 크기가 (3 × 1)임을 알 수 있습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "ocB6asUu_gl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "선형 회귀란 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일입니다.\n",
        "그리고 가장 잘 맞는 직선을 정의하는 것은 바로 W와 b입니다.\n",
        "결론적으로 선형 회귀의 목표는 가장 잘 맞는 직선을 정의하는 W와 b의 값을 찾는 것입니다.\n",
        "우선 가중치 W를 0으로 초기화하고, 이 값을 출력해보겠습니다."
      ],
      "metadata": {
        "id": "TiEUZ-tO_iL-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN2MsCOxYPFu",
        "outputId": "f556d871-a97c-4ec2-933b-27109fd4cff4"
      },
      "source": [
        "W = torch.zeros(1, requires_grad=True)\n",
        "print(W)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "가중치 W가 0으로 초기화되어있으므로 0이 출력된 것을 확인할 수 있습니다. 위에서 requires_grad=True가 인자로 주어진 것을 확인할 수 있습니다. 이는 이 변수는 학습을 통해 계속 값이 변경되는 변수임을 의미합니다.\n",
        "\n",
        "마찬가지로 편향\n",
        "b도 0으로 초기화하고, 학습을 통해 값이 변경되는 변수임을 명시합니다."
      ],
      "metadata": {
        "id": "KFCuI4Ir_n30"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyzHVrRJYWY8",
        "outputId": "ef1bac03-b3f7-41d1-8ca6-9cd3bc8cbab2"
      },
      "source": [
        "b = torch.zeros(1, requires_grad=True)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재 가중치 W와 b둘 다 0이므로 현 직선의 방정식은 다음과 같습니다.\n",
        "$y = 0 × x + 0$\n",
        "지금 상태에선 x에 어떤 값이 들어가도 가설은 0을 예측하게 됩니다. 즉, 아직 적절한 W와 b의 값이 아닙니다."
      ],
      "metadata": {
        "id": "_8AIXThM_pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치 코드 상으로 직선의 방정식에 해당되는 가설을 선언합니다.\n",
        "\n",
        "$ H(x)= Wx+b$  "
      ],
      "metadata": {
        "id": "QBJ_hijdAddN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svSbMlUiYbcz"
      },
      "source": [
        "hypothesis = x_train * W + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치 코드 상으로 선형 회귀의 비용 함수에 해당되는 평균 제곱 오차를 선언합니다.\n"
      ],
      "metadata": {
        "id": "XjB6WdL9AfHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "cost(W, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[y^{(i)} - H(x^{(i)})\\right]^2\n",
        "$$"
      ],
      "metadata": {
        "id": "mrU-YYjjAlh4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cePGtsoMYfXt",
        "outputId": "a9d18310-ce4a-45c8-dc23-9d7997575eb7"
      },
      "source": [
        "# 앞서 배운 torch.mean으로 평균을 구한다.\n",
        "cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "print(cost)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 경사 하강법을 구현합니다. 아래의 'SGD'는 경사 하강법의 일종입니다. lr은 학습률(learning rate)를 의미합니다.\n",
        "학습 대상인 W와 b가 SGD의 입력이 됩니다."
      ],
      "metadata": {
        "id": "VruMrO5UAqBG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjrNx6RJYiEU"
      },
      "source": [
        "optimizer = optim.SGD([W, b], lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "optimizer.zero_grad()를 실행하므로서 미분을 통해 얻은 기울기를 0으로 초기화합니다. 기울기를 초기화해야만 새로운 가중치 편향에 대해서 새로운 기울기를 구할 수 있습니다.  \n",
        "\n",
        "그 다음 cost.backward() 함수를 호출하면 가중치 W와 편향 b에 대한 기울기가 계산됩니다.  \n",
        "\n",
        "그 다음 경사 하강법 최적화 함수 opimizer의 .step() 함수를 호출하여 인수로 들어갔던 W와 b에서 리턴되는 변수들의 기울기에 학습률(learining rate) 0.01을 곱하여 빼줌으로서 업데이트합니다."
      ],
      "metadata": {
        "id": "MbgYlJwqArnY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeQ658JrYnnb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "outputId": "bf017d48-8b5d-416a-be9e-0e250582bcf0"
      },
      "source": [
        "# gradient를 0으로 초기화\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# 비용 함수를 미분하여 gradient 계산\n",
        "cost.backward()\n",
        "\n",
        "# W와 b를 업데이트\n",
        "optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7a99600492c3>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 비용 함수를 미분하여 gradient 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# W와 b를 업데이트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과적으로 훈련 과정에서 $W$와 $b$는 훈련 데이터와 잘 맞는 직선을 표현하기 위한 적절한 값으로 변화해갑니다."
      ],
      "metadata": {
        "id": "5mDsAkngAuhk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qd-nPMlYrOP",
        "outputId": "eb33e409-f953-4a92-e219-6091f03bc5bd"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# 데이터\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])\n",
        "\n",
        "# 모델 초기화\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr=0.01)\n",
        "\n",
        "nb_epochs = 2000 # 원하는만큼 경사 하강법을 반복\n",
        "for epoch in range(1, nb_epochs + 1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    hypothesis = x_train * W + b\n",
        "\n",
        "    # cost 계산\n",
        "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
        "        ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  100/2000 W: 1.745, b: 0.579 Cost: 0.048403\n",
            "Epoch  200/2000 W: 1.800, b: 0.456 Cost: 0.029910\n",
            "Epoch  300/2000 W: 1.842, b: 0.358 Cost: 0.018483\n",
            "Epoch  400/2000 W: 1.876, b: 0.281 Cost: 0.011421\n",
            "Epoch  500/2000 W: 1.903, b: 0.221 Cost: 0.007058\n",
            "Epoch  600/2000 W: 1.923, b: 0.174 Cost: 0.004361\n",
            "Epoch  700/2000 W: 1.940, b: 0.137 Cost: 0.002695\n",
            "Epoch  800/2000 W: 1.953, b: 0.107 Cost: 0.001665\n",
            "Epoch  900/2000 W: 1.963, b: 0.084 Cost: 0.001029\n",
            "Epoch 1000/2000 W: 1.971, b: 0.066 Cost: 0.000636\n",
            "Epoch 1100/2000 W: 1.977, b: 0.052 Cost: 0.000393\n",
            "Epoch 1200/2000 W: 1.982, b: 0.041 Cost: 0.000243\n",
            "Epoch 1300/2000 W: 1.986, b: 0.032 Cost: 0.000150\n",
            "Epoch 1400/2000 W: 1.989, b: 0.025 Cost: 0.000093\n",
            "Epoch 1500/2000 W: 1.991, b: 0.020 Cost: 0.000057\n",
            "Epoch 1600/2000 W: 1.993, b: 0.016 Cost: 0.000035\n",
            "Epoch 1700/2000 W: 1.995, b: 0.012 Cost: 0.000022\n",
            "Epoch 1800/2000 W: 1.996, b: 0.010 Cost: 0.000014\n",
            "Epoch 1900/2000 W: 1.997, b: 0.008 Cost: 0.000008\n",
            "Epoch 2000/2000 W: 1.997, b: 0.006 Cost: 0.000005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "에포크(Epoch)는 전체 훈련 데이터가 학습에 한 번 사용된 주기를 말합니다.\n",
        "이번 실습의 경우 2,000번을 수행했습니다.\n",
        "\n",
        "최종 훈련 결과를 보면 최적의 기울기 W는 2에 가깝고, b는 0에 가까운 것을 볼 수 있습니다. 현재 훈련 데이터가 x_train은 [[1], [2], [3]]이고 y_train은 [[2], [4], [6]]인 것을 감안하면 실제 정답은 W가 2이고, b가 0이므로 거의 정답을 찾은 셈입니다."
      ],
      "metadata": {
        "id": "hCtx5zGB-3Ug"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrQ1rAuLxmz2"
      },
      "source": [
        "# 2. torch.manual_seed()를 하는 이유"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.manual_seed()를 사용한 프로그램의 결과는 다른 컴퓨터에서 실행시켜도 동일한 결과를 얻을 수 있습니다.  \n",
        "\n",
        "그 이유는 torch.manual_seed()는 난수 발생 순서와 값을 동일하게 보장해준다는 특징때문입니다. 우선 랜덤 시드가 3일 때 두 번 난수를 발생시켜보고, 다른 랜덤 시드를 사용한 후에 다시 랜덤 시드를 3을 사용한다면 난수 발생값이 동일하게 나오는지 보겠습니다."
      ],
      "metadata": {
        "id": "grSMas30_Bli"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-3BW1tkxtKP"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjq69tSaxt-n",
        "outputId": "2c1f8c3a-ee14-47d4-e9dd-a2543963bc20"
      },
      "source": [
        "torch.manual_seed(3)\n",
        "print('랜덤 시드가 3일 때')\n",
        "for i in range(1,3):\n",
        "  print(torch.rand(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "랜덤 시드가 3일 때\n",
            "tensor([0.0043])\n",
            "tensor([0.1056])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "랜덤 시드가 3일때 두 개의 난수를 발생시켰더니 0.0043과 0.1056이 나옵니다. 이제 랜덤 시드값을 바꿔봅시다.\n",
        "\n"
      ],
      "metadata": {
        "id": "rDoWgc_h_DaS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXOOxrUSxu_u",
        "outputId": "088b6778-3c18-467f-e06d-e45d6c239cfa"
      },
      "source": [
        "torch.manual_seed(5)\n",
        "print('랜덤 시드가 5일 때')\n",
        "for i in range(1,3):\n",
        "  print(torch.rand(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "랜덤 시드가 5일 때\n",
            "tensor([0.8303])\n",
            "tensor([0.1261])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.8303과 0.1261이 나옵니다. 이제 다시 랜덤 시드값을 3으로 돌려보겠습니다. 이렇게 하면 프로그램을 다시 처음부터 실행한 것처럼 난수 발생 순서가 초기화됩니다."
      ],
      "metadata": {
        "id": "OnxNG7la_FL0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ5F-olVxxwO",
        "outputId": "a3964a4b-f850-45c9-828e-650826fe89fc"
      },
      "source": [
        "torch.manual_seed(3)\n",
        "print('랜덤 시드가 3일 때')\n",
        "for i in range(1,3):\n",
        "  print(torch.rand(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "랜덤 시드가 3일 때\n",
            "tensor([0.0043])\n",
            "tensor([0.1056])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다시 동일하게 0.0043과 0.1056이 나옵니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "4aN0aBm7_G0S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv22uZ_jxz_b"
      },
      "source": [
        "# 3. optimizer.zero_grad()가 필요한 이유"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있습니다. 예를 들어봅시다."
      ],
      "metadata": {
        "id": "bPD_BT-l-tWy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPEsldbqxyp2",
        "outputId": "f7d3dbc0-5c8b-4f48-9c3c-69e83781d59d"
      },
      "source": [
        "import torch\n",
        "\n",
        "w = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "nb_epochs = 20\n",
        "for epoch in range(1, nb_epochs + 1):\n",
        "\n",
        "  z = 2 * w\n",
        "\n",
        "  z.backward()\n",
        "  print('수식을 w로 미분한 값 : {}'.format(w.grad))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "수식을 w로 미분한 값 : 2.0\n",
            "수식을 w로 미분한 값 : 4.0\n",
            "수식을 w로 미분한 값 : 6.0\n",
            "수식을 w로 미분한 값 : 8.0\n",
            "수식을 w로 미분한 값 : 10.0\n",
            "수식을 w로 미분한 값 : 12.0\n",
            "수식을 w로 미분한 값 : 14.0\n",
            "수식을 w로 미분한 값 : 16.0\n",
            "수식을 w로 미분한 값 : 18.0\n",
            "수식을 w로 미분한 값 : 20.0\n",
            "수식을 w로 미분한 값 : 22.0\n",
            "수식을 w로 미분한 값 : 24.0\n",
            "수식을 w로 미분한 값 : 26.0\n",
            "수식을 w로 미분한 값 : 28.0\n",
            "수식을 w로 미분한 값 : 30.0\n",
            "수식을 w로 미분한 값 : 32.0\n",
            "수식을 w로 미분한 값 : 34.0\n",
            "수식을 w로 미분한 값 : 36.0\n",
            "수식을 w로 미분한 값 : 38.0\n",
            "수식을 w로 미분한 값 : 40.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "계속해서 미분값인 2가 누적되는 것을 볼 수 있습니다. 그렇기 때문에 optimizer.zero_grad()를 통해 미분값을 계속 0으로 초기화시켜줘야 합니다."
      ],
      "metadata": {
        "id": "Cp_-ZC47-0vp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTTmw5ZoOjgi"
      },
      "source": [
        "# 4. 로지스틱 회귀 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I322nuCNxll"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coJCmQfrN3kl",
        "outputId": "23a9635e-a8c2-43a9-f60c-c57b84c5266e"
      },
      "source": [
        "torch.manual_seed(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fab0c7dc3b0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "x_train과 y_train을 텐서로 선언합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "hlDB31BbBdQs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11U-YGL4N5Nx"
      },
      "source": [
        "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
        "y_data = [[0], [0], [0], [1], [1], [1]]\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞서 훈련 데이터를 행렬로 선언하고, 행렬 연산으로 가설을 세우는 방법을 배웠습니다.\n",
        "여기서도 마찬가지로 행렬 연산을 사용하여 가설식을 세울겁니다. x_train과 y_train의 크기를 확인해봅시다."
      ],
      "metadata": {
        "id": "bZEsgQycBbv8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su-HBNHSN8q8",
        "outputId": "b2e65535-7745-4d73-bd62-f5993ed1e29b"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 2])\n",
            "torch.Size([6, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재 x_train은 6 × 2의 크기(shape)를 가지는 행렬이며, y_train은 6 × 1의 크기를 가지는 벡터입니다. x_train을 $X$라고 하고, 이와 곱해지는 가중치 벡터를 $W$라고 하였을 때, $XW$가 성립되기 위해서는 $W$ 벡터의 크기는 2 × 1이어야 합니다. 이제 W와 b를 선언합니다."
      ],
      "metadata": {
        "id": "zPtehhzdBhbs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3Y-kujiN989"
      },
      "source": [
        "W = torch.zeros((2, 1), requires_grad=True) # 크기는 2 x 1\n",
        "b = torch.zeros(1, requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 가설식을 세워보겠습니다. 파이토치에서는 $e^{x}$를 구현하기 위해서 torch.exp(x)를 사용합니다.  \n",
        "이에 따라 행렬 연산을 사용한 가설식은 다음과 같습니다."
      ],
      "metadata": {
        "id": "szgKEW8HBjpT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2zJ8cQEN_9c"
      },
      "source": [
        "hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞서 W와 b는 torch.zeros를 통해 전부 0으로 초기화 된 상태입니다. 이 상태에서 예측값을 출력해봅시다."
      ],
      "metadata": {
        "id": "Y01WKTpdBlh6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHLJgC-IOBfj",
        "outputId": "0ab9a175-cf4d-412b-82a4-6c783e359844"
      },
      "source": [
        "print(hypothesis) # 예측값인 H(x) 출력"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "실제값 y_train과 크기가 동일한 6 × 1의 크기를 가지는 예측값 벡터가 나오는데 모든 값이 0.5입니다.  \n",
        "\n",
        "사실 가설식을 좀 더 간단하게도 구현할 수 있습니다. 이미 파이토치에서는 시그모이드 함수를 이미 구현하여 제공하고 있기 때문입니다. 다음은 torch.sigmoid를 사용하여 좀 더 간단히 구현한 가설식입니다."
      ],
      "metadata": {
        "id": "itfS-S-JBo2M"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XkFu19kOC3E"
      },
      "source": [
        "hypothesis = torch.sigmoid(x_train.matmul(W) + b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞서 구현한 식과 본질적으로 동일한 식입니다. 마찬가지로 W와 b가 0으로 초기화 된 상태에서 예측값을 출력해봅시다."
      ],
      "metadata": {
        "id": "GeFkVy-LBqXg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B89fHH4vOEJf",
        "outputId": "f503d5a1-462c-4d5d-e496-bfdadbef8b7b"
      },
      "source": [
        "print(hypothesis)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞선 결과와 동일하게 y_train과 크기가 동일한 6 × 1의 크기를 가지는 예측값 벡터가 나오는데 모든 값이 0.5입니다.  "
      ],
      "metadata": {
        "id": "k1ot_G0MCA9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 아래의 비용 함수값. 즉, 현재 예측값과 실제값 사이의 cost를 구해보겠습니다.\n",
        "$$\n",
        "cost(W) = -\\frac{1}{n} \\sum_{i=1}^{n} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))]\n",
        "$$\n",
        "우선, 현재 예측값과 실제값을 출력해보겠습니다."
      ],
      "metadata": {
        "id": "Kb_89q-4CHwn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bnbag6_pOFtZ",
        "outputId": "1c5348c2-6daf-49e7-8f65-f125476cb5a2"
      },
      "source": [
        "print(hypothesis)\n",
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000]], grad_fn=<SigmoidBackward0>)\n",
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재 총 6개의 원소가 존재하지만 하나의 샘플. 즉, 하나의 원소에 대해서만 오차를 구하는 식을 작성해보겠습니다."
      ],
      "metadata": {
        "id": "r2fM-faZCNI7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJp83njpOG_F",
        "outputId": "64957eb3-e2cd-4af6-8c55-bed8fcfe498f"
      },
      "source": [
        "-(y_train[0] *torch.log(hypothesis[0]) +\n",
        "  (1-y_train[0]) *torch.log(1-hypothesis[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6931], grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 모든 원소에 대해서 오차를 구해보겠습니다."
      ],
      "metadata": {
        "id": "0gjjtTJACO0o"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuEOT6IaOIfg",
        "outputId": "d2a40afb-deb6-47bb-8f1a-741f962bcb6b"
      },
      "source": [
        "losses = -(y_train * torch.log(hypothesis) +\n",
        "           (1 - y_train) * torch.log(1 - hypothesis))\n",
        "print(losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6931],\n",
            "        [0.6931],\n",
            "        [0.6931],\n",
            "        [0.6931],\n",
            "        [0.6931],\n",
            "        [0.6931]], grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "그리고 이 전체 오차에 대한 평균을 구합니다."
      ],
      "metadata": {
        "id": "tY_WBJaCCQDu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Khm12-7hOJ_Y",
        "outputId": "931976e5-ad1b-47e7-d5fc-4e299256598f"
      },
      "source": [
        "cost = losses.mean()\n",
        "print(cost)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6931, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과적으로 얻은 cost는 0.6931입니다.  \n",
        "\n",
        "지금까지 비용 함수의 값을 직접 구현하였는데, 사실 파이토치에서는 로지스틱 회귀의 비용 함수를 이미 구현해서 제공하고 있습니다.  \n",
        "사용 방법은 torch.nn.functional as F와 같이 임포트 한 후에 F.binary_cross_entropy(예측값, 실제값)과 같이 사용하면 됩니다."
      ],
      "metadata": {
        "id": "YW5KpgyACSjF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_opkTIzEOL_n",
        "outputId": "556db183-bfeb-44e1-f5c7-da9808ae2e1f"
      },
      "source": [
        "F.binary_cross_entropy(hypothesis, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "동일하게 cost가 0.6931이 출력되는 것을 볼 수 있습니다. 모델의 훈련 과정까지 추가한 전체 코드는 아래와 같습니다."
      ],
      "metadata": {
        "id": "MsYmUb4xCU-l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWwB32BJONbv"
      },
      "source": [
        "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
        "y_data = [[0], [0], [0], [1], [1], [1]]\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z37fgZ-mOPIy",
        "outputId": "37e225dd-6a5d-4eeb-b502-1d391b9d08f3"
      },
      "source": [
        "# 모델 초기화\n",
        "W = torch.zeros((2, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr=1)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(1, nb_epochs + 1):\n",
        "    # Cost 계산\n",
        "    hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
        "    cost = -(y_train * torch.log(hypothesis) +\n",
        "             (1 - y_train) * torch.log(1 - hypothesis)).mean()\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, cost.item()\n",
        "        ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  100/1000 Cost: 0.135644\n",
            "Epoch  200/1000 Cost: 0.080964\n",
            "Epoch  300/1000 Cost: 0.058062\n",
            "Epoch  400/1000 Cost: 0.045398\n",
            "Epoch  500/1000 Cost: 0.037327\n",
            "Epoch  600/1000 Cost: 0.031720\n",
            "Epoch  700/1000 Cost: 0.027592\n",
            "Epoch  800/1000 Cost: 0.024422\n",
            "Epoch  900/1000 Cost: 0.021911\n",
            "Epoch 1000/1000 Cost: 0.019871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습이 끝났습니다. 이제 훈련했던 훈련 데이터를 그대로 입력으로 사용했을 때, 제대로 예측하는지 확인해보겠습니다.  \n",
        "현재 W와 b는 훈련 후의 값을 가지고 있습니다. 현재 W와 b를 가지고 예측값을 출력해보겠습니다."
      ],
      "metadata": {
        "id": "JM0x5T2fCXEf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWssyJHFOQ_r",
        "outputId": "b0324523-a9a1-4793-8a50-becee7634660"
      },
      "source": [
        "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
        "print(hypothesis)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.7711e-04],\n",
            "        [3.1636e-02],\n",
            "        [3.9014e-02],\n",
            "        [9.5618e-01],\n",
            "        [9.9823e-01],\n",
            "        [9.9969e-01]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재 위 값들은 0과 1 사이의 값을 가지고 있습니다. 이제 0.5를 넘으면 True, 넘지 않으면 False로 값을 정하여 출력해보겠습니다."
      ],
      "metadata": {
        "id": "s8K5KvC8CaNX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tb2WN7pcOYOg",
        "outputId": "efd60ec3-9b58-4e63-cce9-8faabb6149d4"
      },
      "source": [
        "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [ True],\n",
            "        [ True],\n",
            "        [ True]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "실제값은 [[0], [0], [0], [1], [1], [1]]이므로, 이는 결과적으로 False, False, False, True, True, True와 동일합니다. 즉, 기존의 실제값과 동일하게 예측한 것을 볼 수 있습니다. 훈련이 된 후의 W와 b의 값을 출력해보겠습니다."
      ],
      "metadata": {
        "id": "-4MWMxZJCbm1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFVnA3gwOZpE",
        "outputId": "e6f7ccfa-887d-40b4-c93c-c4ba7048f558"
      },
      "source": [
        "print(W)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3.2521],\n",
            "        [1.5174]], requires_grad=True)\n",
            "tensor([-14.4777], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음 실습 : https://colab.research.google.com/drive/1HpetnJwbu7BQcH4TOLXODLlo2cg5qDRR?usp=sharing"
      ],
      "metadata": {
        "id": "NnlcOio_Dx9Q"
      }
    }
  ]
}